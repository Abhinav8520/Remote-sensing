{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0sar8SgM4f1Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sar8SgM4f1Y",
    "outputId": "1acc08cc-c336-41d4-d37e-c33d47e6bdb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c4fca3d-a63f-4674-80c7-87ac87b5aedd",
   "metadata": {
    "id": "9c4fca3d-a63f-4674-80c7-87ac87b5aedd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import load,dump\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import *\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import pydot\n",
    "from numpy import argmax\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import  pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8185fa-23db-4ec6-935e-2c05cf127904",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb8185fa-23db-4ec6-935e-2c05cf127904",
    "outputId": "5c102bef-84ad-4201-d188-f7a00a3df010",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1176\n",
      "Descriptions: train=1176\n",
      "Photos: train=1176\n",
      "Vocabulary Size: 311\n",
      "Description Length: 24\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\t# process line by line\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\t# skip empty lines\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\t# get the image identifier\n",
    "\t\tidentifier = line.split(' ')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    document = open(filename, 'r', encoding='utf-8').read()\n",
    "    descriptions = dict()\n",
    "    for line in document.strip().split('\\n'):\n",
    "        tokens = line.split()\n",
    "        if len(tokens) < 2:  # Check if the line has at least two elements\n",
    "            print(f\"Skipping problematic line: {line}\")  # Print problematic lines\n",
    "            continue  # Skip this iteration if the condition is not met\n",
    "\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        if image_id in dataset:\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = pickle.load(open(filename, 'rb'))\n",
    "    #print(all_features)\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn array(X1), array(X2), array(y)\n",
    "\n",
    "#data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    while True:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            photo = photos.get(key)\n",
    "            if photo is None:\n",
    "                continue\n",
    "            photo = photo[0]\n",
    "            for desc in desc_list:\n",
    "                seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "                for i in range(1, len(seq)):\n",
    "                    in_seq, out_seq = seq[:i], seq[i]\n",
    "                    in_seq = pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n",
    "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                    # Ensure the output is a tuple of tuples\n",
    "                    yield ((np.array(photo, dtype=np.float32), np.array(in_seq, dtype=np.int32)), np.array(out_seq, dtype=np.float32))\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'drive/My Drive/MLE_NEW/Captions/train_captions.txt'\n",
    "train = load_set(filename)\n",
    "#print(sorted(train))\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('drive/My Drive/MLE_NEW/Captions/train_captions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "#print(train)\n",
    "#all_features = load(open('features_inceptionv3_uc.pkl', 'rb'))\n",
    "#print(all_features)\n",
    "train_features = load_photo_features('drive/My Drive/MLE_NEW/Features/train_features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "#dump(tokenizer,open('tokenizer_resnet152.pkl','wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "## determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03eccf99-1ac4-4a7f-9a71-9793b1e6c66a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03eccf99-1ac4-4a7f-9a71-9793b1e6c66a",
    "outputId": "17c910bb-6aff-47a7-fa34-54572b5e6721",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 252\n",
      "Descriptions: val=252\n",
      "Photos: test=252\n"
     ]
    }
   ],
   "source": [
    "# load val set\n",
    "filename = 'drive/My Drive/MLE_NEW/Captions/val_captions.txt'\n",
    "val = load_set(filename)\n",
    "print('Dataset: %d' % len(val))\n",
    "# descriptions\n",
    "val_descriptions = load_clean_descriptions('drive/My Drive/MLE_NEW/Captions/val_captions.txt', val)\n",
    "print('Descriptions: val=%d' % len(val_descriptions))\n",
    "# photo features\n",
    "val_features = load_photo_features('drive/My Drive/MLE_NEW/Features/val_features.pkl', val)\n",
    "print('Photos: test=%d' % len(val_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef2355cd-2e8b-4952-a622-f76d703636c2",
   "metadata": {
    "id": "ef2355cd-2e8b-4952-a622-f76d703636c2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "\n",
    "#@register_keras_serializable()\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim=max_length,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer)\n",
    "\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Attention, self).get_config()\n",
    "        config.update({\n",
    "            'step_dim': self.step_dim,\n",
    "            'W_regularizer': initializers.serialize(self.W_regularizer),\n",
    "            'b_regularizer': initializers.serialize(self.b_regularizer),\n",
    "            'W_constraint': constraints.serialize(self.W_constraint),\n",
    "            'b_constraint': constraints.serialize(self.b_constraint),\n",
    "            'bias': self.bias\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b27d6c82-89d6-4c53-a1c5-4f47656d442a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b27d6c82-89d6-4c53-a1c5-4f47656d442a",
    "outputId": "1bbfb6b9-3bf7-40d3-bf6b-391e15847182",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_8 (InputLayer)        [(None, 24)]                 0         []                            \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, 2048)]               0         []                            \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)     (None, 24, 256)              79616     ['input_8[0][0]']             \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 256)                  524544    ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " lstm_6 (LSTM)               (None, 24, 256)              525312    ['embedding_3[0][0]']         \n",
      "                                                                                                  \n",
      " repeat_vector_3 (RepeatVec  (None, 24, 256)              0         ['dense_9[0][0]']             \n",
      " tor)                                                                                             \n",
      "                                                                                                  \n",
      " time_distributed_3 (TimeDi  (None, 24, 256)              65792     ['lstm_6[0][0]']              \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 24, 512)              0         ['repeat_vector_3[0][0]',     \n",
      " )                                                                   'time_distributed_3[0][0]']  \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirecti  (None, 24, 512)              1574912   ['concatenate_3[0][0]']       \n",
      " onal)                                                                                            \n",
      "                                                                                                  \n",
      " attention_4 (Attention)     (None, 512)                  536       ['bidirectional_3[0][0]']     \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 311)                  159543    ['attention_4[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2930255 (11.18 MB)\n",
      "Trainable params: 2930255 (11.18 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Concatenate, RepeatVector, TimeDistributed, Bidirectional, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "#from keras.utils.np_utils import to_categorical\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    inputs1 = Input(shape=(2048,))\n",
    "    fe2 = Dense(256, activation='relu')(inputs1)\n",
    "    fe3 = RepeatVector(max_length)(fe2)\n",
    "\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = LSTM(256,return_sequences=True)(se1)\n",
    "    se3 = TimeDistributed(Dense(256,activation='relu'))(se2)\n",
    "\n",
    "    decoder1 = concatenate([fe3, se3])\n",
    "    decoder2 = Bidirectional(LSTM(256,return_sequences=True))(decoder1)\n",
    "    decoder3 = Attention(max_length)(decoder2)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder3)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db5bea8c-a500-49cd-b1f7-7136a78f2155",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "db5bea8c-a500-49cd-b1f7-7136a78f2155",
    "outputId": "5546760a-032d-4cca-a273-bcc83474b3bd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "74/74 [==============================] - 15s 36ms/step - loss: 4.7772\n",
      "Epoch 2/125\n",
      "74/74 [==============================] - 2s 22ms/step - loss: 4.4951\n",
      "Epoch 3/125\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 4.5313\n",
      "Epoch 4/125\n",
      "74/74 [==============================] - 3s 39ms/step - loss: 4.3615\n",
      "Epoch 5/125\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 4.0128\n",
      "Epoch 6/125\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 3.9994\n",
      "Epoch 7/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 3.9961\n",
      "Epoch 8/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 4.0587\n",
      "Epoch 9/125\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 4.0408\n",
      "Epoch 10/125\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 3.8520\n",
      "Epoch 11/125\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 3.5328\n",
      "Epoch 12/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 3.8873\n",
      "Epoch 13/125\n",
      "74/74 [==============================] - 2s 25ms/step - loss: 3.5449\n",
      "Epoch 14/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 3.5953\n",
      "Epoch 15/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 3.4207\n",
      "Epoch 16/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 3.1224\n",
      "Epoch 17/125\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 3.1941\n",
      "Epoch 18/125\n",
      "74/74 [==============================] - 2s 27ms/step - loss: 3.4387\n",
      "Epoch 19/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 3.3051\n",
      "Epoch 20/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 3.2439\n",
      "Epoch 21/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.8974\n",
      "Epoch 22/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 2.9253\n",
      "Epoch 23/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.9224\n",
      "Epoch 24/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 3.0489\n",
      "Epoch 25/125\n",
      "74/74 [==============================] - 2s 28ms/step - loss: 3.0794\n",
      "Epoch 26/125\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 3.1235\n",
      "Epoch 27/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 3.0166\n",
      "Epoch 28/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.8749\n",
      "Epoch 29/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.8243\n",
      "Epoch 30/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 2.6734\n",
      "Epoch 31/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.5829\n",
      "Epoch 32/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.4604\n",
      "Epoch 33/125\n",
      "74/74 [==============================] - 2s 27ms/step - loss: 2.8573\n",
      "Epoch 34/125\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 2.9088\n",
      "Epoch 35/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.6905\n",
      "Epoch 36/125\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 2.4930\n",
      "Epoch 37/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.4649\n",
      "Epoch 38/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.4999\n",
      "Epoch 39/125\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 2.1731\n",
      "Epoch 40/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.3328\n",
      "Epoch 41/125\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 2.0881\n",
      "Epoch 42/125\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 2.2914\n",
      "Epoch 43/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.5169\n",
      "Epoch 44/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.5894\n",
      "Epoch 45/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.3916\n",
      "Epoch 46/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.5413\n",
      "Epoch 47/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.2683\n",
      "Epoch 48/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.1396\n",
      "Epoch 49/125\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 2.2975\n",
      "Epoch 50/125\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 2.3852\n",
      "Epoch 51/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 2.0069\n",
      "Epoch 52/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 1.9885\n",
      "Epoch 53/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 1.9807\n",
      "Epoch 54/125\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 1.7817\n",
      "Epoch 55/125\n",
      "74/74 [==============================] - 2s 24ms/step - loss: 1.9460\n",
      "Epoch 56/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 2.1007\n",
      "Epoch 57/125\n",
      "74/74 [==============================] - 2s 31ms/step - loss: 2.1552\n",
      "Epoch 58/125\n",
      "74/74 [==============================] - 2s 27ms/step - loss: 1.7949\n",
      "Epoch 59/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.7284\n",
      "Epoch 60/125\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 1.9006\n",
      "Epoch 61/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 2.0256\n",
      "Epoch 62/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.5318\n",
      "Epoch 63/125\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 1.6902\n",
      "Epoch 64/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.5196\n",
      "Epoch 65/125\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 1.9111\n",
      "Epoch 66/125\n",
      "74/74 [==============================] - 2s 28ms/step - loss: 1.9671\n",
      "Epoch 67/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 1.7712\n",
      "Epoch 68/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.5785\n",
      "Epoch 69/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.7668\n",
      "Epoch 70/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.6077\n",
      "Epoch 71/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.5974\n",
      "Epoch 72/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.5022\n",
      "Epoch 73/125\n",
      "74/74 [==============================] - 2s 28ms/step - loss: 1.2680\n",
      "Epoch 74/125\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 1.2484\n",
      "Epoch 75/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 1.3804\n",
      "Epoch 76/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.4150\n",
      "Epoch 77/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.4017\n",
      "Epoch 78/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 1.1475\n",
      "Epoch 79/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 1.2441\n",
      "Epoch 80/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 1.2531\n",
      "Epoch 81/125\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 1.2698\n",
      "Epoch 82/125\n",
      "74/74 [==============================] - 2s 30ms/step - loss: 1.4132\n",
      "Epoch 83/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.2733\n",
      "Epoch 84/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.2217\n",
      "Epoch 85/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 1.3126\n",
      "Epoch 86/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.3167\n",
      "Epoch 87/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.0305\n",
      "Epoch 88/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.0928\n",
      "Epoch 89/125\n",
      "74/74 [==============================] - 2s 22ms/step - loss: 0.9023\n",
      "Epoch 90/125\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 1.2603\n",
      "Epoch 91/125\n",
      "74/74 [==============================] - 2s 22ms/step - loss: 1.3735\n",
      "Epoch 92/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.1224\n",
      "Epoch 93/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.9232\n",
      "Epoch 94/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.8954\n",
      "Epoch 95/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.1071\n",
      "Epoch 96/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.8324\n",
      "Epoch 97/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.0658\n",
      "Epoch 98/125\n",
      "74/74 [==============================] - 2s 32ms/step - loss: 0.8864\n",
      "Epoch 99/125\n",
      "74/74 [==============================] - 2s 26ms/step - loss: 0.9389\n",
      "Epoch 100/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 1.1551\n",
      "Epoch 101/125\n",
      "74/74 [==============================] - 2s 20ms/step - loss: 1.1022\n",
      "Epoch 102/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 1.0988\n",
      "Epoch 103/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 1.2780\n",
      "Epoch 104/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.9601\n",
      "Epoch 105/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.9954\n",
      "Epoch 106/125\n",
      "74/74 [==============================] - 2s 34ms/step - loss: 0.9751\n",
      "Epoch 107/125\n",
      "74/74 [==============================] - 2s 29ms/step - loss: 1.0055\n",
      "Epoch 108/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.8848\n",
      "Epoch 109/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.9186\n",
      "Epoch 110/125\n",
      "74/74 [==============================] - 2s 21ms/step - loss: 0.8990\n",
      "Epoch 111/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.8232\n",
      "Epoch 112/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.8983\n",
      "Epoch 113/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.9676\n",
      "Epoch 114/125\n",
      "74/74 [==============================] - 3s 35ms/step - loss: 0.9800\n",
      "Epoch 115/125\n",
      "74/74 [==============================] - 2s 23ms/step - loss: 0.9465\n",
      "Epoch 116/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.8984\n",
      "Epoch 117/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.8386\n",
      "Epoch 118/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.9728\n",
      "Epoch 119/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.7803\n",
      "Epoch 120/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.8093\n",
      "Epoch 121/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.8172\n",
      "Epoch 122/125\n",
      "74/74 [==============================] - 2s 33ms/step - loss: 0.9606\n",
      "Epoch 123/125\n",
      "74/74 [==============================] - 2s 28ms/step - loss: 1.0699\n",
      "Epoch 124/125\n",
      "74/74 [==============================] - 1s 20ms/step - loss: 0.8885\n",
      "Epoch 125/125\n",
      "74/74 [==============================] - 1s 19ms/step - loss: 0.8956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x78110b69ce20>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_dataset(generator_func, descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(2048,), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(max_length,), dtype=tf.int32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(vocab_size,), dtype=tf.float32)\n",
    "    )\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        lambda: generator_func(descriptions, photos, tokenizer, max_length, vocab_size),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "\n",
    "# Prepare the dataset\n",
    "train_dataset = make_dataset(data_generator, train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "train_dataset = train_dataset.batch(16)\n",
    "\n",
    "val_dataset = make_dataset(data_generator, val_descriptions, val_features, tokenizer, max_length, vocab_size)\n",
    "val_dataset = val_dataset.batch(16)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(train_dataset, epochs=125, steps_per_epoch=74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5db81da3-a439-4d40-9faa-8eac29435492",
   "metadata": {
    "id": "5db81da3-a439-4d40-9faa-8eac29435492",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('drive/My Drive/MLE_NEW/Main_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b9e49-83f3-47cd-ad2a-7734005312ed",
   "metadata": {
    "id": "498b9e49-83f3-47cd-ad2a-7734005312ed"
   },
   "source": [
    "# Evaluation BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "285ff655-e308-4467-8086-0e5b187b605e",
   "metadata": {
    "id": "285ff655-e308-4467-8086-0e5b187b605e"
   },
   "outputs": [],
   "source": [
    "#val_dataset = data_generator(val_descriptions, val_features, tokenizer, max_length, vocab_size)\n",
    "val_steps = len(val_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fde866e3-84db-46e9-9281-05b75f27dcfb",
   "metadata": {
    "id": "fde866e3-84db-46e9-9281-05b75f27dcfb"
   },
   "outputs": [],
   "source": [
    "def generate_caption(model, image, tokenizer, max_length):\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([np.array([image]), sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word[yhat] if yhat in tokenizer.index_word else None\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34d1af4c-aee7-406f-be79-00f1c87c2ff6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34d1af4c-aee7-406f-be79-00f1c87c2ff6",
    "outputId": "c4e7f469-22cf-448f-ff13-575c94af36f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-1: 0.709641\n",
      "BLEU-2: 0.627896\n",
      "BLEU-3: 0.574487\n",
      "BLEU-4: 0.524471\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = [], []\n",
    "    # Function to generate captions for the photos\n",
    "    for key, desc_list in descriptions.items():\n",
    "        yhat = generate_caption(model, photos[key][0], tokenizer, max_length)\n",
    "        actual.append([d.split() for d in desc_list])\n",
    "        predicted.append(yhat.split())\n",
    "\n",
    "    # Create a SmoothingFunction object\n",
    "    chencherry = SmoothingFunction()\n",
    "\n",
    "    # Calculate BLEU scores with smoothing\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function=chencherry.method1))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.33, 0.33, 0.33, 0), smoothing_function=chencherry.method1))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=chencherry.method1))\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluate_model(model, val_descriptions, val_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o0uVTSs1AdIp",
   "metadata": {
    "id": "o0uVTSs1AdIp"
   },
   "source": [
    "# CIDEr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a481da47-67b7-47ae-b51f-2559b74fb86b",
   "metadata": {
    "id": "a481da47-67b7-47ae-b51f-2559b74fb86b"
   },
   "outputs": [],
   "source": [
    "generated_captions = {}\n",
    "for img_id, features in val_features.items():\n",
    "    caption = generate_caption(model, features[0], tokenizer, max_length)\n",
    "    generated_captions[img_id] = [caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cQh4ds0XBua_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQh4ds0XBua_",
    "outputId": "e8d905ec-360e-4360-fe21-4c8c93d95a1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/salaniz/pycocoevalcap\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap to /tmp/pip-req-build-p_drvink\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/salaniz/pycocoevalcap /tmp/pip-req-build-p_drvink\n",
      "  Resolved https://github.com/salaniz/pycocoevalcap to commit a24f74c408c918f1f4ec34e9514bc8a76ce41ffd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from pycocoevalcap==1.2) (2.0.7)\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (1.25.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.16.0)\n",
      "Building wheels for collected packages: pycocoevalcap\n",
      "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312246 sha256=11f89a148df0eead7365edee3bc4360cdb3594c21e957cde041d53433de06947\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-bqw36rx0/wheels/43/54/73/3e2c6d4ace7657958cde52ac6fd47b342cd4aae5a7aa4fcbf9\n",
      "Successfully built pycocoevalcap\n",
      "Installing collected packages: pycocoevalcap\n",
      "Successfully installed pycocoevalcap-1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/salaniz/pycocoevalcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f29547b-6a3e-4112-bac1-428559ac6cca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f29547b-6a3e-4112-bac1-428559ac6cca",
    "outputId": "8216cba6-122e-47ed-fd90-77af69257531"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIDEr Score:  2.385882867433643\n"
     ]
    }
   ],
   "source": [
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def calculate_cider(refs, hyps):\n",
    "    cider_scorer = Cider()\n",
    "    score, scores = cider_scorer.compute_score(refs, hyps)\n",
    "    return score\n",
    "\n",
    "refs = {img_id: desc for img_id, desc in val_descriptions.items()}\n",
    "cands = generated_captions\n",
    "\n",
    "cider_score = calculate_cider(refs, cands)\n",
    "print(\"CIDEr Score: \", cider_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cde78b-0d6a-43db-8fae-fa72c06d6f69",
   "metadata": {
    "id": "26cde78b-0d6a-43db-8fae-fa72c06d6f69"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
